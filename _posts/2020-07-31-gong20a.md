---
title: A Duality Approach for Regret Minimization in Average-Award Ergodic Markov
  Decision Processes
abstract: In light of the Bellman duality, we propose a novel value-policy gradient
  algorithm to explore and act in infinite-horizon Average-reward Markov Decision
  Process (AMDP) and show that it has sublinear regret. The algorithm is motivated
  by the Bellman saddle point formulation. It learns the optimal state-action distribution,
  which encodes a randomized policy, by interacting with the environment along a single
  trajectory and making primal-dual updates. The key to the analysis is to establish
  a connection between the min-max duality gap of Bellman saddle point and the cumulative
  regret of the learning agent. We show that, for ergodic AMDPs with finite state
  space $\mathcal{S}$ and action space $\mathcal{A}$ and uniformly bounded mixing
  times, the algorithmâ€™s $T$-time step regret is $$ R(T)=\tilde{\mathcal{O}}\left(
  \left(t_{mix}^*\right)^2 \tau^{\frac{3}{2}} \sqrt{(\tau^3 + |\mathcal{A}|) |\mathcal{S}|
  T} \right), $$ where $t_{mix}^*$ is the worst-case mixing time, $\tau$ is an ergodicity
  parameter, $T$ is the number of time steps and $\tilde{\mathcal{O}}$ hides polylog
  factors.
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: gong20a
month: 0
tex_title: A Duality Approach for Regret Minimization in Average-Award Ergodic Markov
  Decision Processes
firstpage: 862
lastpage: 883
page: 862-883
order: 862
cycles: false
bibtex_author: Gong, Hao and Wang, Mengdi
author:
- given: Hao
  family: Gong
- given: Mengdi
  family: Wang
date: 2020-07-31
address: 
publisher: PMLR
container-title: Proceedings of the 2nd Conference on Learning for Dynamics and Control
volume: '120'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 31
pdf: http://proceedings.mlr.press/v120/gong20a/gong20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
