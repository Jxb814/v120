---
title: Model-Based Reinforcement Learning with Value-Targeted Regression
abstract: 'Reinforcement learning (RL) applies to control problems with large state
  and action spaces, hence it is natural to consider RL with a parametric model. In
  this paper we focus on finite-horizon episodic RL where the transition model admits
  the linear parametrization: $P = \sum_{i=1}^{d} (\theta)_{i}P_{i}$. This parametrization
  provides a universal function approximation and capture several useful models and
  applications. We propose an upper confidence model-based RL algorithm with value-targeted
  model parameter estimation. The algorithm updates the estimate of $\theta$ by recursively
  solving a regression problem using the latest value estimate as the target. We demonstrate
  the efficiency of our algorithm by proving its expected regret bound $\tilde{\mathcal{O}}(d\sqrt{H^{3}T})$,
  where $H, T, d$ are the horizon, total number of steps and dimension of $\theta$.
  This regret bound is independent of the total number of states or actions, and is
  close to a lower bound $\Omega(\sqrt{HdT})$.'
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: jia20a
month: 0
tex_title: Model-Based Reinforcement Learning with Value-Targeted Regression
firstpage: 666
lastpage: 686
page: 666-686
order: 666
cycles: false
bibtex_author: Jia, Zeyu and Yang, Lin and Szepesvari, Csaba and Wang, Mengdi
author:
- given: Zeyu
  family: Jia
- given: Lin
  family: Yang
- given: Csaba
  family: Szepesvari
- given: Mengdi
  family: Wang
date: 2020-07-31
address: 
publisher: PMLR
container-title: Proceedings of the 2nd Conference on Learning for Dynamics and Control
volume: '120'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 31
pdf: http://proceedings.mlr.press/v120/jia20a/jia20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
