---
title: Periodic Q-Learning
abstract: The use of target networks is a common practice in deep reinforcement learning
  for stabilizing the training; however, theoretical understanding of this technique
  is still limited. In this paper, we study the so-called periodic Q-learning algorithm
  (PQ-learning for short), which resembles the technique used in deep Q-learning for
  solving infinite-horizon discounted Markov decision processes (DMDP) in the tabular
  setting. PQ-learning maintains two separate Q-value estimates â€“ the online estimate
  and target estimate. The online estimate follows the standard Q-learning update,
  while the target estimate is updated periodically. In contrast to the standard Q-learning,
  PQ-learning enjoys a simple finite time analysis and achieves better sample complexity
  for finding an epsilon-optimal policy. Our result provides a preliminary justification
  of the effectiveness of utilizing target estimates or networks in Q-learning algorithms.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: lee20a
month: 0
tex_title: Periodic Q-Learning
firstpage: 582
lastpage: 598
page: 582-598
order: 582
cycles: false
bibtex_author: Lee, Donghwan and He, Niao
author:
- given: Donghwan
  family: Lee
- given: Niao
  family: He
date: 2020-07-31
address: 
publisher: PMLR
container-title: Proceedings of the 2nd Conference on Learning for Dynamics and Control
volume: '120'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 31
pdf: http://proceedings.mlr.press/v120/lee20a/lee20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
