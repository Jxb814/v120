---
title: Linear Antisymmetric Recurrent Neural Networks
abstract: 'Recurrent Neural Networks (RNNs) have a form of memory where the output
  from a node at one timestep is fed back as input the next timestep in addition to
  data from the previous layer. This makes them highly suitable for timeseries analysis.
  However, standard RNNs have known weaknesses such as exploding/vanishing gradient
  and thereby struggle with a long-term memory. In this paper, we suggest a new recurrent
  network structure called Linear Antisymmetric RNN (LARNN). This structure is based
  on the numerical solution to an Ordinary Differential Equation (ODE) with stability
  properties resulting in a stable solution, which corresponds to long-term memory
  and trainability. Three different numerical methods are suggested to solve the ODE:
  Forward and Backward Euler and the midpoint method. The suggested structure has
  been implemented in Keras and several simulated datasets have been used to evaluate
  the performance. In the investigated cases, the LARNN performs better or similar
  to the Long Short Term Memory (LSTM) network which is the current state of the art
  for RNNs.'
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: moe20a
month: 0
tex_title: Linear Antisymmetric Recurrent Neural Networks
firstpage: 170
lastpage: 178
page: 170-178
order: 170
cycles: false
bibtex_author: Moe, Signe and Remonato, Filippo and Gr{\o}tli, Esten Ingar and Gravdahl,
  Jan Tommy
author:
- given: Signe
  family: Moe
- given: Filippo
  family: Remonato
- given: Esten Ingar
  family: Gr√∏tli
- given: Jan Tommy
  family: Gravdahl
date: 2020-07-31
address: 
publisher: PMLR
container-title: Proceedings of the 2nd Conference on Learning for Dynamics and Control
volume: '120'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 31
pdf: http://proceedings.mlr.press/v120/moe20a/moe20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
