---
title: Scalable Reinforcement Learning of Localized Policies for Multi-Agent Networked
  Systems
abstract: 'We study reinforcement learning (RL) in a setting with a network of agents
  whose states and actions interact in a local manner where the objective is to find
  localized policies such that the (discounted) global reward is maximized. A fundamental
  challenge in this setting is that the state-action space size scales exponentially
  in the number of agents, rendering the problem intractable for large networks. In
  this paper, we propose a Scalable Actor Critic (SAC) framework that exploits the
  network structure and finds a localized policy that is an $O(\rho^\kappa)$-approximation
  of a stationary point of the objective for some $\rho\in(0,1)$, with complexity
  that scales with the local state-action space size of the largest $\kappa$-hop neighborhood
  of the network. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: qu20a
month: 0
tex_title: Scalable Reinforcement Learning of Localized Policies for Multi-Agent Networked
  Systems
firstpage: 256
lastpage: 266
page: 256-266
order: 256
cycles: false
bibtex_author: Qu, Guannan and Wierman, Adam and Li, Na
author:
- given: Guannan
  family: Qu
- given: Adam
  family: Wierman
- given: Na
  family: Li
date: 2020-07-31
address: 
publisher: PMLR
container-title: Proceedings of the 2nd Conference on Learning for Dynamics and Control
volume: '120'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 31
pdf: http://proceedings.mlr.press/v120/qu20a/qu20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
