---
title: 'Contracting Implicit Recurrent Neural Networks: Stable Models with Improved
  Trainability'
abstract: Stability of recurrent models is closely linked with trainability, generalizability
  and in some applications,safety. Methods that train stable recurrent neural networks,
  however, do so at a significant cost to expressibility. We propose an implicit model
  structure that allows for a convex parametrization of stable models using contraction
  analysis of non-linear systems. Using these stability conditions we propose a new
  approach to model initialization and then provide a number of empirical results
  comparing the performance of our proposed model set to previous stable RNNs and
  vanilla RNNs. By carefully controlling stability in the model, we observe a significant
  increase in the speed of training and model performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: revay20a
month: 0
tex_title: 'Contracting Implicit Recurrent Neural Networks: Stable Models with Improved
  Trainability'
firstpage: 393
lastpage: 403
page: 393-403
order: 393
cycles: false
bibtex_author: Revay, Max and Manchester, Ian
author:
- given: Max
  family: Revay
- given: Ian
  family: Manchester
date: 2020-07-31
address: 
publisher: PMLR
container-title: Proceedings of the 2nd Conference on Learning for Dynamics and Control
volume: '120'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 31
pdf: http://proceedings.mlr.press/v120/revay20a/revay20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
