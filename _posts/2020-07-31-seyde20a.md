---
title: Learning to Plan via Deep Optimistic Value Exploration
abstract: Deep exploration requires coordinated long-term planning. We present a model-based
  reinforcement learning algorithm that guides policy learning through a value function
  that exhibits optimism in the face of uncertainty. We capture uncertainty over values
  by combining predictions from an ensemble of models and formulate an upper confidence
  bound (UCB) objective to recover optimistic estimates. Training the policy on ensemble
  rollouts with the learned value function as the terminal cost allows for projecting
  long-term interactions into a limited planning horizon, thus enabling deep optimistic
  exploration. We do not assume a priori knowledge of either the dynamics or reward
  function. We demonstrate that our approach can accommodate both dense and sparse
  reward signals, while improving sample complexity on a variety of benchmarking tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: seyde20a
month: 0
tex_title: Learning to Plan via Deep Optimistic Value Exploration
firstpage: 815
lastpage: 825
page: 815-825
order: 815
cycles: false
bibtex_author: Seyde, Tim and Schwarting, Wilko and Karaman, Sertac and Rus, Daniela
author:
- given: Tim
  family: Seyde
- given: Wilko
  family: Schwarting
- given: Sertac
  family: Karaman
- given: Daniela
  family: Rus
date: 2020-07-31
address: 
publisher: PMLR
container-title: Proceedings of the 2nd Conference on Learning for Dynamics and Control
volume: '120'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 31
pdf: http://proceedings.mlr.press/v120/seyde20a/seyde20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
