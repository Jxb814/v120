---
title: Improving Robustness via Risk Averse Distributional Reinforcement Learning
abstract: One major obstacle that precludes the success of reinforcement learning
  in real-world applications is the lack of robustness, either to model uncertainties
  or external disturbances, of the trained policies. Robustness is critical when the
  policies are trained in simulations instead of real world environment. In this work,
  we propose a risk-aware algorithm to learn robust policies in order to bridge the
  gap between simulation training and real-world implementation. Our algorithm is
  based on recently discovered distributional RL framework. We incorporate CVaR risk
  measure in sample based distributional policy gradients (SDPG) for learning risk-averse
  policies to achieve robustness against a range of system disturbances. We validate
  the robustness of risk-aware SDPG on multiple environments.
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: singh20a
month: 0
tex_title: Improving Robustness via Risk Averse Distributional Reinforcement Learning
firstpage: 958
lastpage: 968
page: 958-968
order: 958
cycles: false
bibtex_author: Singh, Rahul and Zhang, Qinsheng and Chen, Yongxin
author:
- given: Rahul
  family: Singh
- given: Qinsheng
  family: Zhang
- given: Yongxin
  family: Chen
date: 2020-07-31
address: 
publisher: PMLR
container-title: Proceedings of the 2nd Conference on Learning for Dynamics and Control
volume: '120'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 31
pdf: http://proceedings.mlr.press/v120/singh20a/singh20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
