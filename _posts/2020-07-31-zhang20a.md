---
title: 'Policy Optimization for $\mathcal{H}_2$ Linear Control with $\mathcal{H}_\infty$ Robustness
  Guarantee: Implicit Regularization and Global Convergence'
abstract: Policy optimization (PO) is a key ingredient for modern reinforcement learning
  (RL). For control design, certain constraints are usually enforced on the policies
  to optimize, accounting for either the stability, robustness, or safety concerns
  on the system. Hence, PO is by nature a constrained (nonconvex) optimization in
  most cases, whose global convergence is challenging to analyze in general. More
  importantly, some constraints that are safety-critical, e.g., the closed-loop stability,
  or the $\mathcal{H}_{\infty}$-norm constraint that guarantees the system robustness,
  can be difficult to enforce on the controller being learned as the PO methods proceed.
  In this paper, we study the convergence theory of PO for $\mathcal{H}_{2}$ linear
  control with $\mathcal{H}_{\infty}$ robustness guarantee. This general framework
  includes risk-sensitive linear control as a special case. One significant new feature
  of this problem, in contrast to the standard $\mathcal{H}_{2}$ linear control, namely,
  linear quadratic regulator (LQR) problems, is the lack of coercivity of the cost
  function. This makes it challenging to guarantee the feasibility, namely, the $\mathcal{H}_{\infty}$
  robustness, of the iterates. Interestingly, we propose two PO algorithms that enjoy
  the implicit regularization property, i.e., the iterates preserve the $\mathcal{H}_{\infty}$
  robustness, as if they are regularized by the algorithms. Furthermore, convergence
  to the globally optimal policies with globally sublinear and locally (super-)linear
  rates are provided under certain conditions, despite the nonconvexity of the problem.
  To the best of our knowledge, our work offers the first results on the implicit
  regularization property and global convergence of PO methods for robust/risk-sensitive
  control.
layout: inproceedings
series: Proceedings of Machine Learning Research
issn: 2640-3498
id: zhang20a
month: 0
tex_title: 'Policy Optimization for $\mathcal{H}_{2}$ Linear Control with $\mathcal{H}_{âˆž}$
  Robustness Guarantee: Implicit Regularization and Global Convergence'
firstpage: 179
lastpage: 190
page: 179-190
order: 179
cycles: false
bibtex_author: Zhang, Kaiqing and Hu, Bin and Basar, Tamer
author:
- given: Kaiqing
  family: Zhang
- given: Bin
  family: Hu
- given: Tamer
  family: Basar
date: 2020-07-31
address: 
publisher: PMLR
container-title: Proceedings of the 2nd Conference on Learning for Dynamics and Control
volume: '120'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 31
pdf: http://proceedings.mlr.press/v120/zhang20a/zhang20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
