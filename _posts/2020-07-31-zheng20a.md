---
title: Constrained Upper Confidence Reinforcement Learning
abstract: Constrained Markov Decision Processes are a class of stochastic decision
  problems in which the decision maker must select a policy that satisfies auxiliary
  cost constraints. This paper extends upper confidence reinforcement learning for
  settings in which the reward function and the  constraints, described by cost functions,
  are unknown a priori but the transition kernel is known. Such a setting is well-motivated
  by a number of applications including exploration of unknown, potentially unsafe,
  environments. We present an algorithm C-UCRL and show that it achieves sub-linear
  regret with respect to the reward while satisfying the constraints even while learning
  with high probability. An illustrative example is provided.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zheng20a
month: 0
tex_title: Constrained Upper Confidence Reinforcement Learning
firstpage: 620
lastpage: 629
page: 620-629
order: 620
cycles: false
bibtex_author: Zheng, Liyuan and Ratliff, Lillian
author:
- given: Liyuan
  family: Zheng
- given: Lillian
  family: Ratliff
date: 2020-07-31
address: 
publisher: PMLR
container-title: Proceedings of the 2nd Conference on Learning for Dynamics and Control
volume: '120'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 31
pdf: http://proceedings.mlr.press/v120/zheng20a/zheng20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
